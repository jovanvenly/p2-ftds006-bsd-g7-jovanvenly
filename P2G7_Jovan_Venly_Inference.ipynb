{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "\n",
    "# Introduction\n",
    "\n",
    "---\n",
    "\n",
    "Name : Jovan Venly J Runturambi  \n",
    "Batch : 007\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement :   \n",
    "\n",
    "Im Data scientist and i want information about case problem in company amazon to understand customer satisfaction levels and the impact of comments on products on the Amazon website, analyzing review data using Natural Language Processing (NLP) is crucial. By applying NLP techniques, we can analyze customer reviews to measure satisfaction levels based on the sentiment contained in the review text, as well as evaluate how positive and negative comments affect product perception. This technique allows us to identify the sentiment patterns underlying customer feedback, understand the factors that contribute to satisfaction or dissatisfaction, and measure the impact of review sentiment on product reputation and success in the marketplace.\n",
    "\n",
    "Objective :   \n",
    "\n",
    "evaluate customers' satisfaction levels by identifying whether their reviews are positive, negative, or neutral. In addition, we wanted to understand how comments, both positive and negative, affect the view and judgment of the product in the `NLP` process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Load Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tf_hub\n",
    "\n",
    "# Library Pre-Processing\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Models\n",
    "model = load_model('model_ltsm1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Stopwords\n",
    "additional_stopwords = ['to', 'I','the','a','my','and','i', 'you', 'is', 'for', 'in', 'of',\n",
    " 'it', 'on', 'have', 'that', 'me', 'so', 'with', 'be', 'but',\n",
    " 'at', 'was', 'just', 'I`m', 'not', 'get', 'all', 'this', 'are',\n",
    " 'out', 'like', 'day', '-', 'up', 'go', 'your', 'good', 'got', 'from',\n",
    " 'do', 'going', 'no', 'now', 'love', 'work', '****', 'will', 'about',\n",
    " 'one', 'really', 'it`s', 'u', 'don`t', 'some', 'know', 'see', 'can',\n",
    " 'too', 'had', 'am', 'back', '&', 'time', 'what', 'its', 'want', 'we',\n",
    " 'new', 'as', 'im', 'think', 'can`t', '2', 'if', 'when', 'an', 'more',\n",
    " 'still', 'today', 'miss', 'has', 'they', 'much', 'there', 'last',\n",
    " 'need', 'My', 'how', 'been', 'home', 'lol', 'off', 'Just', 'feel',\n",
    " 'night', 'i`m', 'her', 'would', 'The']\n",
    "\n",
    "# Setting stopwords english\n",
    "stpwds_eng = list(set(stopwords.words('english')))\n",
    "for i in additional_stopwords:\n",
    "    stpwds_eng.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat fungsi cleaning\n",
    "cleaning_pattern = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stpwds_eng = list(set(stopwords.words('english')))\n",
    "\n",
    "# build text cleaning function\n",
    "def text_preprocessing(text):\n",
    "\n",
    "    # Mengubah text ke Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Menghilangkan mention, link, dan karakter non-alfanumerik\n",
    "    text = re.sub(cleaning_pattern, ' ', text)\n",
    "\n",
    "    # Menghilangkan Mention\n",
    "    text = re.sub(\"@[A-Za-z0-9_]+\", \" \", text)\n",
    "\n",
    "    # Menghilangkan Hashtag\n",
    "    text = re.sub(\"#[A-Za-z0-9_]+\", \" \", text)\n",
    "\n",
    "    # Menghilangkan \\n (newline)\n",
    "    text = re.sub(r\"\\\\n\", \" \",text)\n",
    "\n",
    "    # Menghilangkan kata dibawah 3 character\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', \" \",text)\n",
    "\n",
    "    # URL removal\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"www.\\S+\", \" \", text)\n",
    "\n",
    "    # Menghilangkan Whitespace di awal dan akhir\n",
    "    text = text.strip()\n",
    "\n",
    "    # Non-letter removal (such as emoticon, symbol (like μ, $, 兀), etc\n",
    "    text = re.sub(\"[^A-Za-z\\s']\", \" \", text)\n",
    "\n",
    "    # Menghilangkan double space\n",
    "    text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "    # Melakukan Tokenisasi\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Menghilangkan Stopwords\n",
    "    text = ' '.join([word for word in tokens if word not in stpwds_eng])\n",
    "\n",
    "    # Melakukan Lemmatizer\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New Data \n",
    "\n",
    "data_inf = {\n",
    "    'text' : '''\n",
    "hello, what's up brother? today i go to supermarket.\n",
    "    '''}\n",
    "\n",
    "data_inf = pd.DataFrame([data_inf])\n",
    "\n",
    "# show new data\n",
    "data_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf['reviewText'] = data_inf['reviewText'].apply(lambda x: text_preprocessing(x))\n",
    "data_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf['text'] = data_inf['text'].apply(lambda x: text_preprocessing(x))\n",
    "data_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using ANN\n",
    "y_pred_inf = model.predict(data_inf)\n",
    "y_pred_inf = np.argmax(y_pred_inf)\n",
    "if y_pred_inf == 0:\n",
    "    print(f'That is negative Tweet')\n",
    "elif y_pred_inf == 1:\n",
    "    print(f'That is neutral Tweet')\n",
    "elif y_pred_inf == 2:\n",
    "    print(f'That is positive Tweet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haktiv8_batch005",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
